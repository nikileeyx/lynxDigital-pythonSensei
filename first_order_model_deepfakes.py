# -*- coding: utf-8 -*-
"""first_order_model_deepfakes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/JaumeClave/deepfakes_first_order_model/blob/master/first_order_model_deepfakes.ipynb

# Deepfakes with the First Order Model Method
Creating uncanny deepfakes of famous sports stars by Jaume Clave  
September 26th, 2020

## Index
[Deepfakes](#Deepfakes)  

[Evolution of the Deepfake](#Evolution-of-the-Deepfake)

[Creating a Deepfake](#Creating-a-Deepfake)  
i. [Extraction](#Extraction)  
ii. [Training](#Training)  
iii. [Creation](#Creation)  

[First Order Model for Image Automation](#First-Order-Model-for-Image-Automation)  
i. [Motion Extraction](#Motion-Extraction)  
ii. [Generation Module](#Generation-Module)  
iii. [Exectuing the Model](#Exectuing-the-Model)  

[Neural Networks](#Neural-Networks)

[Autoencoders](#Autoencoders)

[Running on Custom Data](#Running-on-Custom-Data)

[Conclusion](#Conclusion)

[Further Reading](#Further-Reading)

## Deepfakes <a name="Deepfakes"></a> 
A deepfake, coming from the words *deep learning* and *fake*, is a synthetic media in which a person in an existing image or video is replaced with someone else's likeness. Deepfakes make use of powerful techniques from machine learning and artifical intellegence to manipulate and generate visual and audio content with a high potential to decieve. The underlying mechanism for deepfake creation is deep learning models such as autoencoders and generative adverisal networks (GAN). These models are used to to examine facial expressions and movements of a person and syntesixe facial images of another person making analogous expressions and movements. 

Deepfakes have garnered widespread attention for their uses in celebrity pornographic videos, revenge porn, fake news, hoaxes, and financial fraud. This has elicited responses from both industry and government to detect and limit their use. This paper will explore what deepfakes are, what machine learning algorithms are used to create a deepfake animation and how they can be run.

## Evolution of the Deepfake <a name="Evolution-of-the-Deepfake"></a> 
In the late 90s and early 2000s face detection was a major area of research because of its possible implecations for military and security use. Almost twenty years later, this problem is basically solved and face detection technology is available freely as open-source libraries in most programming languages. Pythons most popular face detection library may be OpenCV or face-recognition. 

From here various apps have evolved to be able to swap faces of two people in images. Friends have used these apps to see how they would look with the other friends body and even switched faces with celebrities and politicians. 


<img src = 'https://drive.google.com/uc?id=1bR9F10B3V78dU_xAWDu6Om_3LpbbqDcJ' height = 200>

<img src = 'https://drive.google.com/uc?id=1M7dcH0CaR0Ok-JBoEOJaghwEAYRbkHZd' height = 200>

In the first set of images the *face alignment* process is initialized. Left: Detected facial landmarks and convex hull. Middle: Delaunay triangulation of points on convex hull. Right: Face alignment by affine warping triangles.

In the second set of images the face-swap process is continued. The dst image is image onto which we want to blend the source image (i.e. the image of Donald Trump ). The mask is calculated by filling the convex hull with white using fillConvexPoly and the center is the center of the bounding box that contains the mask.

Swaping faces programatically by warping and color correcting one faces to the others body may create good fakes but it has one huge disadvantage: it only works on pre-exisitning pictures. It cannot morph Face A to match the expression in Face B.  This changed in 2017 when a new approach to face-swap appeared on Reddit. This breakthrough relied on neural networks to generate these deepfakes which actually morophed a person's face to mimic someone else's features, all while perserving the original facial expression.

## Creating a Deepfake <a name="Creating-a-Deepfake"></a> 
The deepfake creation process requires three steps: extraction, training and creation. This section will explain, at a top-level, what each of these stages mean and what they do in relation to the entire process.

### Extraction <a name="Extraction"></a> 
Deepfakes leverage deep neural networks to transform faces and require large amounts of data (images) to make it all work smoothly and believable. The extraction process refers to the step of extracting all frames from video clips, identifying the faces and aligning them to optimze for perfomance. 

<img src = 'https://drive.google.com/uc?id=1-PR60_fm-gF58AIkB3cUb-rH2osgiVxa' height = 200>

The alignment is a critical step since the neural network that performs the faceswap requires all faces to have the same size (usualyl 256 x 256) and aligned features. Detecting and aligning faces is a problem that is considered mostly solved, and is done by most applications very efficiently (face detection).

### Training <a name="Training"></a> 
The training stage allows the neural network to convert a face into another. The training may take several hours or even days depending on the size of the training set and the device the model is trained on. Like training most other neural networks the training only needs to be completed once. Once the model is trained it would be able to convert a face from person A to person B.

<img src = 'https://drive.google.com/uc?id=1VekvM6foF0Xjzk4LP7uCV5hGlVYUj5Zs' >


This is the most technical step in the deepfake creation process and is explained in greater technical detail further into the paper.

### Creation <a name="Creation"></a> 
Once the model is trained, a deepfake may be created. Starting from a video, frames are extracted and all faces are aligned. Each frame is then converted using the trained neural network. The final step is to merge the converted face back to the original frame. 

<img src = 'https://drive.google.com/uc?id=17l_Q1w3mDqmqfmBDDVOVA4PkYs2UwSAK' >

The creation stage is the only one which does not make use of any machine learning algorithms. The process is to stich a face back onto an image is hard-coded and thus lacks the ability to detect mistakes.

<img src = 'https://drive.google.com/uc?id=1asptwgEiDOWxclpsQgwQf2bb5w6qKPDy' >

Each frame is also processed independitly which means there is no temporal correlation. A temporal correlation function is a function that gives the statistical correlation between random variable, contingent on the temporal (time) distance between those variables. The lack of this correlation results in the final video having some flickering. This is the part of deepfakes which requires the most amount of research in.

## First Order Model for Image Automation <a name="First-Order-Model-for-Image-Automation"></a> 
The problme thus far, when it came to building deepfakes, was the need for additional information. For example, if the movement of the head was to be mapped there was a need for specific subject facial landmarks and full-body mappings required subject pose-estimation.  In 2019 a research team from the University of Toronto introduced a paper called *First Order Model for Image Automation* at the NeuraIPS conference. This paper presented an impressive way to animate a source image given a driving video, without any additional information or annotation about the object to automate. This method outperfomed state of the art on all the benchmarks and works well on a variety of images. Apart from this, once the model was trained, it can be used for transfer learning and it can be applied to an arbitraty object of the same category. The source code for the project can be found on [Aliaksandr Siarohin Github Repo](https://aliaksandrsiarohin.github.io/first-order-model-website/)

This section explores the model the research team presented, explains how it works and clones it for use in this paper.
"""

## Mount Github
from google.colab import drive
drive.mount('/content/gdrive')

## Clone the repo
!pip install PyYAML==5.3.1
!git clone https://github.com/AliaksandrSiarohin/first-order-model

cd first-order-model

"""Working in Google Colab is beneficial for deep learning as Google gives access to its GPUs for free. Another benefit is the ability to mount a Google Drive to the cloud Virtual Machine (VM). This allows the user to access all his content easily. This section will contain the code required to mount Google Drive to the cloud VM.

### Motion Extraction <a name="Motion-Extraction"></a> 
The purpose of the motion estimation module is to predict a dense motion field. The model assumes there exists an abstract reference frame and it independently estimates two transformations: from reference to source and from reference to driving. This choice allows the model to independently process source and driving frames. This is desired since, at test time the model receives pairs of the source image and driving frames sampled from a different video, which can be very different visually.

The motion extractor utilizes an autoencoder to detect keypoints and extracts first-order motion representation that consists of sparse keypoints and local affine transformations. 


<img src = 'https://drive.google.com/uc?id=1e_aM4XLvLkXBlx7ItJckix-tZRFGbJeO' >

### Generation Module <a name="Generation-Module"></a> 
Finally, the generation module renders an image of the source object moving as provided in the driving video. Here, the model uses a generator network that warps the source image according to dense motion and inpaints the image parts that are occluded in the source image.

The motion estimator learns the latent representations of the motion in the video. These representations are encoded as motion-specific key point displacements (where key points can be the position of eyes or mouth) and local affine transformations. The output of the model is two-fold: a dense motion field and an occlusion mask. This mask defines which parts of the driving video can be reconstructed by warping the source image. The context needs to infer parts that are not present in the source image, such as the back of the person in the fashion example, where the person’s back is not presented in the source picture.

The video generator output of the motion detector and the source image and animates it according to the driving video. It warps that source image in ways that resemble the driving video and the occluded parts.

### Exectuing the Model <a name="Exectuing-the-Model"></a> 
This section will use the OOB model to show how the model works. This is a vanilla example which takes a static image of Putin (source image)and a video of Obama (driving video). The output is a video of Putin, saying and vocalising with the exact facial expresions Obama moves with from the driving video.

First the media will be loaded and functions will be declared which will help display the output of the model. The model will then be created and checkpoints will be loaded. Finally the deepfake will be created and two types of animation will be shows.
"""

## Import modules
import imageio
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from skimage.transform import resize
from IPython.display import HTML
import warnings
warnings.filterwarnings("ignore")

## Load source and drive


## Resize image and video to 256x256

## Create model
from demo import load_checkpoints
generator, kp_detector = load_checkpoints(config_path='config/vox-256.yaml', 
                            checkpoint_path='/content/gdrive/My Drive/python_projects/deepfake/vox-cpk.pth.tar')

"""Relative keypoint displacment is used to animate Putin using Obama's motion. It is impressive to see how the expressions and movements Obama has and uses during his videos are animated well and clearly for Putin. There are some slight errors, especially when Obama lifts his eyebrows and opens his eyes. Putins frames do not perfectly mimic those expresions. However, if the Putin video was seen on TV or Social Media without the deepfake context it would look quite believable and real."""

## Import modules
from demo import make_animation
from skimage import img_as_ubyte


## Save resulting video

## Video can be downloaded from /content folder

"""In the cell above relative keypoint displacement is used to animate the objects. The following cell makes use of absolute coordinates instead, but in this way all the object proporions will be inherited from the driving video."""

## Absolute coordinates
predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=False, adapt_movement_scale=True)
HTML(display(source_image, driving_video, predictions).to_html5_video())

"""## Neural Networks <a name="Neural-Networks"></a> 
Neural networks are computational systems loosely inspored by the way in which the brain processes information. Special cells called neurons are connected to each other in a dense network, which allow information to be processed and transmitted. In computer science, artificial neural networks are made out of thousands of nodes, connectred in specific fashion. These nodes are typically arranged in layers; the way in which they are connected determines the type of the network and ultimately, its ability to perform a certain computational task over another one. 

<img src = 'https://drive.google.com/uc?id=1nhDO_BYIsyT163bpALey0DeUkU3gs5BL' >

Each node or neuron in the *input layer* contains a numerical value that encodes the input that will be fed to the network.

For example, if the network is trying to predict the weather, the input nodes might contain the pressure, temperature, humidity and wind speed encoded as numbers in the [-1, +1] range. These values are then broadcast to the next layer; each edge dampens or amplifies the values it transmits. Each node sums all the values it recieves, and outputs a new one based on its own function. The result of the computation can be retrieved from the output layer; in this case, only one value is prodced. 

#### ANNs with Images
When images are the input (or output) of a neural network, typically the network has three input nodes for each pixel, initialised with the amount of red, green and blue it contains. The most effective architecture for image-based applications so far are convolutional neural networks (CNN), which is exactly what deepfake models leverage. 

Training a neural network means finding a set of weights for all edges, so that the output layer produces the desired result. One of the most used technique to achieve this is called backpropagation, and it works by re-adjusting the weights every time the network makes a mistake.

The basic idea behind face detection and image generation is that each layer will represent progressively core complex features. In the case of a face, for instance, the first layer might detect edges, the second face features, which the third layer is able to use to detect images.

<img src = 'https://drive.google.com/uc?id=1Jem-FWW-yD7NR7gRWc_eyJwdlhaCDAu7'>

## Autoencoders <a name="Autoencoders"></a> 
An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. This section will explain what an autoencoder does and its importance to a deepfake model.

The network below has two fully connected hidden layers, with four neurons each.

<img src = 'https://drive.google.com/uc?id=1hGCxK-Aqca6NGptc7YUYR2eL8fFv84f0' >

If this network is trained as an autoencoder, a problem may arise. Edges might converge to a solution where the input values are simply transported into their respective output nodes, as seen in the diagram below. When this happens, no real learning is happening; the network has rewired itself to simply connect the output nodes to the input ones.

<img src = 'https://drive.google.com/uc?id=1Iy4c6PJmKHzMhBlsISpZH3kQPYwjR8uF' >

However, if one of the layers has fewer nodes than the other, the input values cannot be simply connected to their respective output nodes. In these situations, the autoencoder has to somehow compress the information provided and it must reconstruct it before presenting it as its final output. 

<img src = 'https://drive.google.com/uc?id=1rKjWFle05fb1hcsNaDaGfCevwGkm9kM-' >

If the training is succesful, the autoencoder has learned how to represent the input values in a different, yet more compact form. The autoencoder can be decoupled into two separate networks: an encoder and a decoder, both sharing the layer in the middle. The values $[Y_0, Y_1]$ are often referred to as base vector, and they represent the input image in the so-called latent space.

Autoencoders are naturally lossy, meaning that they will not be able to reconstruct the input image perfectly. However, because the autoencoder is forced to reconstruct the input image as best as it can, it has to learn how to identify and to represents its most meaningful features. Because the smaller details are often ignored or lost, an autoencoder can be used to denoise images.

## Further Reading <a name="Further-Reading"></a> 
#### Deepfakes 
https://en.wikipedia.org/wiki/Deepfake    
https://arxiv.org/pdf/1909.11573.pdf   
https://www.kaggle.com/gpreda/deepfake-starter-kit    

#### Deeplearning
http://deeplearning.net/   
https://machinelearningmastery.com/what-is-deep-learning/    
https://www.mathworks.com/discovery/deep-learning.html    

#### First Order Method Model
https://github.com/AliaksandrSiarohin/first-order-model    
https://aliaksandrsiarohin.github.io/first-order-model-website/  
http://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation
"""